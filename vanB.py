import os
import io
import base64
import tempfile
import streamlit as st
from typing import List, Tuple, Optional

APP_NAME = "Ludvig Van Beethoven"
INDEX_DIR = "./data/index/faiss"
UPLOAD_DIR = "./data/uploads"
DEFAULT_TEMPERATURE = 0.2

os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(os.path.dirname(INDEX_DIR), exist_ok=True)

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langchain.schema import Document

from langchain_ollama.chat_models import ChatOllama

from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader

try:
    import pytesseract
    from PIL import Image
    OCR_AVAILABLE = True
except Exception:
    OCR_AVAILABLE = False


st.set_page_config(page_title=f"ğŸ¼ {APP_NAME} â€“ RAG Chat", layout="wide")
st.title(f"ğŸ¼ {APP_NAME}")
st.caption("ğŸ“š Dosya/Resim yÃ¼kle, indeksle ve RAG ile sorular sor. Ä°ndeks yoksa normal sohbet baÅŸlar.")

with st.sidebar:
    st.subheader("âš™ï¸ Ayarlar")
    top_k = st.slider("Retriever k (kaÃ§ parÃ§a getirilsin)", 2, 10, 4, 1)
    chunk_size = st.number_input("Chunk size", 256, 4000, 1200, 50)
    chunk_overlap = st.number_input("Chunk overlap", 0, 800, 200, 10)

    st.markdown("---")
    st.markdown("### ğŸ“¦ VektÃ¶r Ä°ndeksi")
    reset_index = st.button("Ä°ndeksi SÄ±fÄ±rla (Sil)")
    if reset_index:
        try:
            if os.path.exists(INDEX_DIR):
                for item in os.listdir(INDEX_DIR):
                    item_path = os.path.join(INDEX_DIR, item)
                    if os.path.isfile(item_path):
                        os.remove(item_path)
            st.success("Ä°ndeks iÃ§eriÄŸi silindi.")
        except Exception as e:
            st.error(f"Silinirken hata: {e}")

    if st.button("ğŸ’¬ Sohbeti SÄ±fÄ±rla"):
        st.session_state.messages = []
        st.rerun()

# DEÄÄ°ÅÄ°KLÄ°K 2: get_embeddings fonksiyonu OllamaEmbeddings kullanacak ÅŸekilde gÃ¼ncellendi
def get_embeddings():
    # RAG iÃ§in popÃ¼ler ve Ã¼cretsiz Ollama gÃ¶mme modeli kullanÄ±lÄ±r.
    # LÃ¼tfen Ollama'da bu modelin kurulu olduÄŸundan emin olun: 'ollama pull nomic-embed-text'
    model_name = "nomic-embed-text" 
    
    # OllamaEmbeddings ile baÅŸlatma
    ollama_embeddings = OllamaEmbeddings(
        model=model_name
    )
    return ollama_embeddings

def load_llm() -> object:
    temp = DEFAULT_TEMPERATURE 
    return ChatOllama(model="gpt-oss:20b", temperature=temp)

def load_documents_from_files(uploaded_files: List[st.runtime.uploaded_file_manager.UploadedFile]) -> List[Document]:
    """PDF/TXT/DOCX/IMG dosyalarÄ±ndan Document listesi Ã¼retir."""
    docs: List[Document] = []
    for f in uploaded_files:
        file_bytes = f.read()
        suffix = os.path.splitext(f.name)[1].lower()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
            tmp_file.write(file_bytes)
            path = tmp_file.name

        try:
            if suffix in [".pdf"]:
                loader = PyPDFLoader(path)
            elif suffix in [".txt", ".md", ".csv", ".py", ".json"]:
                loader = TextLoader(path, encoding="utf-8")
            elif suffix in [".docx"]:
                loader = Docx2txtLoader(path)
            else:
                loader = None

            if loader:
                ld = loader.load()
                for d in ld:
                    d.metadata = d.metadata or {}
                    d.metadata.update({"source": f.name})
                docs.extend(ld)

            elif suffix in [".png", ".jpg", ".jpeg", ".webp"]:
                text = ""
                if OCR_AVAILABLE:
                    try:
                        image = Image.open(io.BytesIO(file_bytes))
                        text = pytesseract.image_to_string(image)
                    except Exception as e:
                        st.warning(f"OCR hatasÄ± ({f.name}): {e}")
                else:
                    st.info("OCR etkin deÄŸil yani pytesseract yÃ¼klÃ¼ deÄŸil. GÃ¶rsellerden metin Ã§Ä±karmak iÃ§in pytesseract kurulmalÄ±.")

                if text.strip():
                    docs.append(Document(page_content=text, metadata={"source": f.name, "type": "image_ocr"}))
            else:
                st.warning(f"Desteklenmeyen dosya tÃ¼rÃ¼: {f.name}")
        finally:
            if os.path.exists(path):
                os.unlink(path)

    return docs

def build_or_update_index(all_docs: List[Document], chunk_size=1200, chunk_overlap=200) -> Optional[FAISS]:
    if not all_docs and not os.path.exists(INDEX_DIR):
        return None

    splitter = CharacterTextSplitter(separator="\n", chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = splitter.split_documents(all_docs)
    
    embeddings = get_embeddings()

    if os.path.exists(INDEX_DIR) and os.listdir(INDEX_DIR):
        try:
            vs = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
            if chunks:
                vs.add_documents(chunks)
                vs.save_local(INDEX_DIR)
            return vs
        except Exception:
            if not chunks:
                 return None
            pass

    if chunks:
        # FAISS.afrom_documents yerine FAISS.from_documents kullanÄ±ldÄ± (async olmadÄ±ÄŸÄ± iÃ§in)
        vs = FAISS.from_documents(chunks, embeddings)
        vs.save_local(INDEX_DIR)
        return vs
        
    return None

def load_index() -> Optional[FAISS]:
    if not os.path.exists(INDEX_DIR) or not os.listdir(INDEX_DIR):
        return None
    embeddings = get_embeddings()
    try:
        if os.path.exists(INDEX_DIR) and os.listdir(INDEX_DIR):
            return FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)
        return None
    except Exception:
        return None

def make_system_prompt(is_rag: bool) -> str:
    if is_rag:
        return (
            f"Sen {APP_NAME} adlÄ± bir yardÄ±mcÄ± asistansÄ±n. "
            "KullanÄ±cÄ±nÄ±n yÃ¼klediÄŸi dÃ¶kÃ¼manlardan ilgili parÃ§alarÄ± al ve kaynak gÃ¶stererek yanÄ±t ver. "
            "CevabÄ±n kÄ±sa, doÄŸru ve TÃ¼rkÃ§e olsun. KaynaklarÄ± sonunda madde halinde listele. "
            "EÄŸer baÄŸlamda soruyla ilgili bilgi bulamÄ±yorsan, genel bilgi ile cevap ver ve 'Kaynaklar:' kÄ±smÄ±nÄ± ekleme."
        )
    else:
        return (
            f"Sen {APP_NAME} adlÄ± bir yardÄ±mcÄ± asistansÄ±n. "
            "KÄ±sa, doÄŸru ve akÄ±cÄ± TÃ¼rkÃ§e cevaplar ver. BaÄŸlamda bilgi bulamadÄ±ÄŸÄ±n iÃ§in genel bilgi ile yanÄ±t veriyorsun."
        )

def format_history_for_prompt(messages: List[Tuple[str, str]]) -> str:
    """Streamlit mesajlarÄ±nÄ± LLM prompt'u iÃ§in formatlar."""
    history = []
    for role, content in messages:
        if role == "user":
            history.append(f"KullanÄ±cÄ±: {content}")
        elif role == "assistant":
            clean_content = content.split('\n**Kaynaklar:**')[0].strip()
            history.append(f"Asistan: {clean_content}")
    
    return "\n".join(history[:-1])

def rag_answer(llm, vectorstore: FAISS, query: str, history: str, k: int = 4) -> Tuple[str, List[Document]]:
    """RAG kullanarak cevap Ã¼retir."""
    retriever = vectorstore.as_retriever(search_kwargs={"k": k})
    docs = retriever.invoke(query)
    context = "\n\n---\n\n".join([f"[{i+1}] {d.page_content}" for i, d in enumerate(docs)])
    sys = make_system_prompt(is_rag=True)

    prompt = (
        f"{sys}\n\n"
        f"KONUÅMA GEÃ‡MÄ°ÅÄ°:\n{history}\n\n"
        f"BAÄLAM (en alakalÄ± parÃ§alar):\n{context}\n\n"
        f"KULLANICI SORUSU:\n{query}\n\n"
        "YanÄ±t formatÄ±:\n"
        "- KÄ±sa ve dÃ¶kÃ¼manlara dayalÄ± bir cevap paragrafÄ±\n"
        "- EÄŸer dÃ¶kÃ¼man kullanÄ±ldÄ±ysa: Kaynaklar: [1] DosyaAdÄ±, [2] ... ÅŸeklinde liste\n"
        "NOT: EÄŸer baÄŸlam alakasÄ±zsa veya boÅŸsa, dÃ¶kÃ¼manlara dayalÄ± olmadan cevap ver ve kaynak gÃ¶sterme."
    )
    resp = llm.invoke(prompt)
    return resp.content, docs

def chat_answer(llm, query: str, history: str) -> str:
    """Sadece chat geÃ§miÅŸini kullanarak cevap Ã¼retir (RAG yokken)."""
    sys = make_system_prompt(is_rag=False)
    
    prompt = (
        f"{sys}\n\n"
        f"KONUÅMA GEÃ‡MÄ°ÅÄ°:\n{history}\n\n"
        f"KULLANICI SORUSU:\n{query}\n\n"
        "KÄ±sa ve ilgili cevabÄ±nÄ± ver."
    )
    resp = llm.invoke(prompt)
    return resp.content

def render_sources(docs: List[Document]):
    if not docs:
        return
    unique_sources = {}
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source", "bilinmiyor")
        unique_sources[src] = unique_sources.get(src, []) + [i]

    st.markdown("**Kaynaklar:**")
    for src in unique_sources:
        st.write(f"- {src}")

st.subheader("ğŸ“¥ Dosya/Resim YÃ¼kle")
uploads = st.file_uploader(
    "PDF, TXT, DOCX, PNG, JPG ekleyebilirsin (birden fazla seÃ§)",
    type=["pdf", "txt", "md", "csv", "py", "json", "docx", "png", "jpg", "jpeg", "webp"],
    accept_multiple_files=True
)

col_a, col_b = st.columns(2)
with col_a:
    if st.button("ğŸ§± Ä°ndeksi GÃ¼ncelle / OluÅŸtur"):
        with st.spinner("Ä°ndeks gÃ¼ncelleniyor..."):
            new_docs = load_documents_from_files(uploads) if uploads else []
            vs = build_or_update_index(new_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap) 
            
            if new_docs:
                st.success(f"Ä°ndeks hazÄ±r! {len(new_docs)} yeni belge eklendi/gÃ¼ncellendi.")
            elif vs:
                st.info("Yeni dosya yok. Mevcut indeks yÃ¼klendi.")
            else:
                st.warning("Ã–nce en az bir dosya yÃ¼kleyin veya mevcut indeks bulunamadÄ±.")

with col_b:
    vs_test = load_index()
    st.info("Ä°ndeks durumu: " + ("âœ… YÃ¼klÃ¼ (RAG Etkin)" if vs_test else "âŒ Yok (Normal Sohbet)"))

st.markdown("---")

st.subheader("ğŸ’¬ Soru Sor")
if "messages" not in st.session_state:
    st.session_state.messages = []

for role, content in st.session_state.messages:
    with st.chat_message(role):
        st.markdown(content)

query = st.chat_input("Sorunu yazâ€¦")

if query:
    with st.chat_message("user"):
        st.markdown(query)
    st.session_state.messages.append(("user", query))
    
    history = format_history_for_prompt(st.session_state.messages)

    vs = load_index()
    llm = load_llm()

    with st.chat_message("assistant"):
        with st.spinner("YanÄ±t hazÄ±rlanÄ±yor..."):
            try:
                if vs:
                    answer, used_docs = rag_answer(llm, vs, query, history, k=top_k)
                    st.markdown(answer)

                    if any("Kaynaklar:" in line for line in answer.split('\n')):
                        render_sources(used_docs)
                else:
                    answer = chat_answer(llm, query, history)
                    st.markdown(answer)
                
                st.session_state.messages.append(("assistant", answer))
            except Exception as e:
                st.error(f"Hata: {e}")
